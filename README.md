# Real-TIme-ASL-converter

ASL Image-to-Text Neural Network Model

There are approximately 250,000-500,00 people in the United States that use American Sign Language. There is a constant communication gap for those who have hearing impairment and use sign language to communicate. This lack of understanding is something that can isolate such people, and it is important that there be mechanisms for people who are unable to converse in the same language, have certain ways that they can still communicate with one another. From this train of thought, we decided to develop a machine learning model that recognizes and processes English numbers and alphabets in American sign language, and converts them to text. 

We have approached this problem using a feedforward or fully connected neural network model within a Keras framework in which we implemented data augmentation, and used machine learning packages such as Tensorflow and Mediapipe to process the input images and detect hand signs in real-time. 

Goal:
The goal of this project is to develop a feedforward or fully connected neural network model that converts American Sign Language portrayed from real-time images into text. This model aims for a 75 to 80% accuracy level in recognising alphabets and letters, and a 95 to 100% success rate in processing all the input image data.

Dataset Link - https://www.kaggle.com/datasets/ayuraj/asl-dataset
